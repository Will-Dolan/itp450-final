{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-22 10:38:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt.4'\n",
      "\n",
      "100%[======================================>] 1,115,394   --.-K/s   in 0.05s   \n",
      "\n",
      "2024-11-22 10:38:56 (20.9 MB/s) - 'input.txt.4' saved [1115394/1115394]\n",
      "\n",
      "----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "----\n",
      "Length of dataset: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Let's download the tiny shakespeare dataset from Karpathy's github!\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Let's load the dataset into memory\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# let's look at the first 100 characters\n",
    "print('----')\n",
    "print(text[:100])\n",
    "\n",
    "# let's print the length of the dataset\n",
    "print('----')\n",
    "print(f\"Length of dataset: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize the dataset (Character-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "---\n",
      "Vocabulary size: 65 characters\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(set(text))\n",
    "vocab_size=len(chars)\n",
    "\n",
    "print(chars)\n",
    "print('---')\n",
    "print(f\"Vocabulary size: {vocab_size} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to map characters to integers and vice versa\n",
    "c2i={c:i for i, c in enumerate(chars)} # character to integer\n",
    "i2c={i:c for i, c in enumerate(chars)} # integer to character\n",
    "\n",
    "# encode a string to a \"list of integers\"\n",
    "encode=lambda s: [c2i[c] for c in s]\n",
    "\n",
    "# decode a list of integers to a string\n",
    "decode=lambda l: ''.join([i2c[i] for i in l])\n",
    "\n",
    "# Let's test it out\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=torch.Size([1115394]), data.dtype=torch.int64\n",
      "---\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset (and put it in a pytorch tensor)\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'data.shape={data.shape}, data.dtype={data.dtype}')\n",
    "print('---')\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a) Let's split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% - training set, 10% - validation set\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b) Let's see how to create input and targets during the training\n",
    "The goal of training a transformer language model is to predict \"the next token in the sequence\" given \"the current sequence of tokens\".\n",
    "\n",
    "**Example:**\n",
    "`Hello World! I am ChatGPT!`\n",
    "\n",
    "**Prediction:**\\\n",
    "Input: A sequence of tokens -> Output: The next token in the sequence\n",
    "___\n",
    "##### *Approach 1:*\n",
    "**Prediction 1:**\n",
    "Input: `[H, E, L, L, O, W, O, R]` -> Output: `[L]`\n",
    "___\n",
    "##### *Approach 2 (Sliding Window):*    \n",
    "**Prediction 1:**\n",
    "Input: `[H]` -> Output: `[L]`\n",
    "\n",
    "**Prediction 2:**\n",
    "Input: `[H, E]` -> Output: `[L]`\n",
    "\n",
    "**Prediction 3:**\n",
    "Input: `[H, E, L]` -> Output: `[L]`\n",
    "\n",
    "...\n",
    "\n",
    "**Prediction 8:**\n",
    "Input: `[H, E, L, L, O, W, O, R]` -> Output: `[L]`\n",
    "\n",
    "The sliding window approach:\n",
    "- Creates multiple training examples from a single input\n",
    "- While the sequence length is fixed, this teaches the model to handle contexts of different lengths -> critical for flexibility of the model\n",
    "- Can be done in parallel without pulling new data from the memory\n",
    "- Because of progressive learning, the training becomes more smooth and stable.\n",
    "- Improves the model understanding of contextual relationships between tokens\n",
    "\n",
    "___\n",
    "\n",
    "So, here to implement the sliding window, we create the input and output via the format below:\n",
    "- **Input:**`[H, E, L, L, O, W, O, R]`\n",
    "- **Target:** `[E, L, L, O, W, O, R, L]`\n",
    "\n",
    "We can see how this is actually implemented in the self-attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "seq_size=8, batch_size=4\n",
      "context.shape=torch.Size([4, 8]), target.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Define the sequence size\n",
    "# These are just small values for testing, later we will use a larger value\n",
    "seq_size   = 8 # how many characters are in a sequence\n",
    "batch_size = 4 # how many sequences in a batch\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # Randomly select starting indices for the batch (0, len(data)-seq_size)\n",
    "    # Do it for batch_size number of times and store in a torch vector\n",
    "    start_ind = torch.randint(0,len(data)-seq_size,(batch_size,))\n",
    "    #print(f'start_ind={start_ind}')\n",
    "    \n",
    "    context=[]\n",
    "    target=[]\n",
    "    for i in start_ind:\n",
    "        context.append(data[i  :i+seq_size  ])\n",
    "        target.append (data[i+1:i+seq_size+1])\n",
    "    \n",
    "    context = torch.stack(context) #converts from a list of tensors to a large tensor\n",
    "    target  = torch.stack(target)\n",
    "\n",
    "    return context, target\n",
    "\n",
    "context,target=get_batch('train')\n",
    "\n",
    "# Printing some properties of what we built\n",
    "print('---')\n",
    "print(f'seq_size={seq_size}, batch_size={batch_size}')\n",
    "print(f'context.shape={context.shape}, target.shape={target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=tensor([[ 1, 53, 44,  1, 58, 46, 63,  1],\n",
      "        [32, 47, 57,  1, 40, 43, 39, 59],\n",
      "        [39, 63,  1, 58, 53,  1, 25, 53],\n",
      "        [10,  0, 20, 43,  5, 57,  1, 39]])\n",
      "target =tensor([[53, 44,  1, 58, 46, 63,  1, 49],\n",
      "        [47, 57,  1, 40, 43, 39, 59, 58],\n",
      "        [63,  1, 58, 53,  1, 25, 53, 61],\n",
      "        [ 0, 20, 43,  5, 57,  1, 39,  1]])\n"
     ]
    }
   ],
   "source": [
    "print(f'context={context}')\n",
    "print(f'target ={target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: \" of thy \"\n",
      "  --> o\n",
      " o --> f\n",
      " of -->  \n",
      " of  --> t\n",
      " of t --> h\n",
      " of th --> y\n",
      " of thy -->  \n",
      " of thy  --> k\n",
      "---in numbers---\n",
      "input: tensor([1]) --> output: 53\n",
      "input: tensor([ 1, 53]) --> output: 44\n",
      "input: tensor([ 1, 53, 44]) --> output: 1\n",
      "input: tensor([ 1, 53, 44,  1]) --> output: 58\n",
      "input: tensor([ 1, 53, 44,  1, 58]) --> output: 46\n",
      "input: tensor([ 1, 53, 44,  1, 58, 46]) --> output: 63\n",
      "input: tensor([ 1, 53, 44,  1, 58, 46, 63]) --> output: 1\n",
      "input: tensor([ 1, 53, 44,  1, 58, 46, 63,  1]) --> output: 49\n"
     ]
    }
   ],
   "source": [
    "b=0 #for example, at a fixed batch id\n",
    "print(f'sequence: \"{decode(context[b,:].tolist())}\"')\n",
    "for t in range(seq_size):\n",
    "    input_seq = context[b,:t+1].tolist()\n",
    "    output_seq=[target[b,t].tolist()]\n",
    "    print(f'{decode(input_seq)} --> {decode(output_seq)}')\n",
    "\n",
    "print('---in numbers---')\n",
    "for t in range(seq_size):\n",
    "    print(f'input: {context[b,:t+1]} --> output: {target[b,t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**From here, we will start building the blocks of the transformer!**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/transformer.png\" width=\"200\" alt=\"Transformer\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "*Input context* has the shape [Batch, Sequence Length] or `[B,T]`\n",
    "\n",
    "*Embedding tensor* would be of shape [Batch, Sequence Length, Channel] or `[B,T,C]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding\n",
    "Here, we are using a simple trainable embedding. It is essentially a look up table that for each \"token\", stores the \"embedding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        #\n",
    "        # nn.Embedding is a lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # It takes two arguments:\n",
    "        # 1. the size of the dictionary of embeddings: vocab_size  ; here we have 65 different tokens\n",
    "        # 2. the size of each embedding vector: embed_dim          ; how many numbers are used to describe each token\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    # taking the input tensor and returning the embedded tensor\n",
    "    def forward(self,input_context):\n",
    "        embedding_table=self.token_embedding_table(input_context)\n",
    "        #\n",
    "        print(f'input_contex.shape={input_context.shape}')\n",
    "        print(f'embedding_table.shape={embedding_table.shape}')\n",
    "\n",
    "        return embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_contex.shape=torch.Size([4, 8])\n",
      "embedding_table.shape=torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary in the our text (all unique characters, words, ...)\n",
    "# Determined earlier vocab_size=65\n",
    "\n",
    "# Embedding dimension\n",
    "embed_dim=65\n",
    "\n",
    "# using above class to create the embedding\n",
    "org_embedding    = TokenEmbedding(vocab_size,embed_dim)\n",
    "org_embd_context = org_embedding(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Self attention\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/sa.png\" width=\"150\" alt=\"Self Attention\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inputs to the SelfAttention:* Embedded context `[B,T,C]`\n",
    "\n",
    "1- Create **linear transformation matrices** for **Key, Query, Value** matrices.\\\n",
    "*Note:* We build the $W_K$, $W_Q$, $W_V$ matrices for all tokens in **a sequence**.\\\n",
    "These are of shape `[C,C]` and applied in parallel across the batch.\\\n",
    "\n",
    "2- Use **Query** and **Key** matrices to create the base attention score\\\n",
    "*Note:* $Q$ = $W_Q$ * (Embedded context), $K$ = $W_K$ * (Embedded context)\\\n",
    "$Q$ and $K$ have shape `[B,T,C] @ [C,C] = [B,T,C]`\n",
    "\n",
    "base_scores = `Q @ K.T` will be of dimension: `[B,T,C]*[B,C,T] = [B,T,T]`\n",
    "\n",
    "3- Scale it with the $\\sqrt{d_{emb}}$, where $d_{emb}$ in the code is referred to as `C` (Channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Let's talk about Masking:** Let's revisit our input before entering the transformer block. Imagine B=1:\n",
    "\n",
    "context input: `[H, E, L, L, O, W, O, R]`\\\n",
    "target output: `[E, L, L, O, W, O, R, L]`\n",
    "\n",
    "```\n",
    "H -> [0.1, 0.2, ..., 0.0]  # C=65 numbers\\\n",
    "E -> [0.3, 0.1, ..., 0.2]  # C=65 numbers\\\n",
    "context = [\n",
    "    [0.1, 0.2, ..., 0.0],  # H\n",
    "    [0.3, 0.1, ..., 0.2],  # E\n",
    "    [...],                  # L\n",
    "    [...],                  # L\n",
    "    [...],                  # O\n",
    "    [...],                  # W\n",
    "    [...],                  # O\n",
    "    [...],                  # R\n",
    "]  # shape: [B=1, T=8, C=65]\n",
    "\n",
    "Q = context @ Wq  # [1, 8, 65]\n",
    "K = context @ Wk  # [1, 8, 65]\n",
    "\n",
    "scores = [\n",
    "    # H    E    L    L    O    W    O    R\n",
    "    [H·H, H·E, H·L, H·L, H·O, H·W, H·O, H·R],  # H's interactions\n",
    "    [E·H, E·E, E·L, E·L, E·O, E·W, E·O, E·R],  # E's interactions\n",
    "    [L·H, L·E, L·L, L·L, L·O, L·W, L·O, L·R],  # L's interactions\n",
    "    [L·H, L·E, L·L, L·L, L·O, L·W, L·O, L·R],  # L's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O, O·R],  # O's interactions\n",
    "    [W·H, W·E, W·L, W·L, W·O, W·W, W·O, W·R],  # W's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O, O·R],  # O's interactions\n",
    "    [R·H, R·E, R·L, R·L, R·O, R·W, R·O, R·R],  # R's interactions\n",
    "]  # shape: [1, 8, 8] or [B, T, T]\n",
    "```\n",
    "Remember that we want to have predictions for the next token via a sliding window approach:\n",
    "- At step 0 (first row), we want the input to be `[H]` and the target to be `[E]`\n",
    "- At step 1 (second row), we want the input to be `[H, E]` and the target to be `[L]`\n",
    "- ...\n",
    "- At step 7 (last row), we want the input to be `[H, E, L, L, O, W, O, R]` and the target to be `[L]`\n",
    "\n",
    "So, we essentially want the scores to be like:\n",
    "```\n",
    "scores = [\n",
    "    # H    E    L    L    O    W    O    R\n",
    "    [H·H, 0  ,  0  , 0  , 0  , 0  , 0  , 0 ],  # H's interactions\n",
    "    [E·H, E·E,  0  , 0  , 0  , 0  , 0  , 0 ],  # E's interactions\n",
    "    [L·H, L·E, L·L,  0  , 0  , 0  , 0  , 0 ],  # L's interactions\n",
    "    [L·H, L·E, L·L, L·L,  0  , 0  , 0  , 0 ],  # L's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O,  0  , 0  , 0 ],  # O's interactions\n",
    "    [W·H, W·E, W·L, W·L, W·O, W·W,  0  , 0 ],  # W's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O,  0 ],  # O's interactions\n",
    "    [R·H, R·E, R·L, R·L, R·O, R·W, R·O, R·R],  # R's interactions\n",
    "]  # shape: [1, 8, 8] or [B, T, T]\n",
    "```\n",
    "This is why we need to mask the future tokens. Instead of setting them to 0, we set them to $-\\infty$ so that after the softmax, they become 0.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4- Masking the future tokens with $-\\infty$ \\\n",
    "5- Softmax:\n",
    "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "6- Multiply by Value matrix to create the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.shape,k.shape= torch.Size([4, 8, 65]) torch.Size([4, 8, 65])\n",
      "k_transposed.shape= torch.Size([4, 65, 8])\n",
      "---\n",
      "scores at B=0, before masking: \n",
      " tensor([[ 0.3780,  0.0206,  0.5184,  0.3780, -0.1899,  0.0797, -0.3345,  0.3780],\n",
      "        [ 0.8497, -0.3750,  0.8537,  0.8497, -0.4762,  0.0776, -0.3316,  0.8497],\n",
      "        [-0.1210,  0.3485,  0.1177, -0.1210,  0.0648,  0.2108, -0.0094, -0.1210],\n",
      "        [ 0.3780,  0.0206,  0.5184,  0.3780, -0.1899,  0.0797, -0.3345,  0.3780],\n",
      "        [-0.5623,  0.1195,  0.2031, -0.5623, -0.2597, -0.5136,  0.3023, -0.5623],\n",
      "        [ 0.2551, -0.1633, -0.1260,  0.2551,  0.4417, -0.0251,  0.3760,  0.2551],\n",
      "        [-0.1449, -0.0945,  0.2616, -0.1449,  0.0036, -0.1592,  0.4848, -0.1449],\n",
      "        [ 0.3780,  0.0206,  0.5184,  0.3780, -0.1899,  0.0797, -0.3345,  0.3780]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " ---\n",
      "scores at B=0, after masking: \n",
      " tensor([[ 0.3780,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.8497, -0.3750,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1210,  0.3485,  0.1177,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.3780,  0.0206,  0.5184,  0.3780,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.5623,  0.1195,  0.2031, -0.5623, -0.2597,    -inf,    -inf,    -inf],\n",
      "        [ 0.2551, -0.1633, -0.1260,  0.2551,  0.4417, -0.0251,    -inf,    -inf],\n",
      "        [-0.1449, -0.0945,  0.2616, -0.1449,  0.0036, -0.1592,  0.4848,    -inf],\n",
      "        [ 0.3780,  0.0206,  0.5184,  0.3780, -0.1899,  0.0797, -0.3345,  0.3780]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " ---\n",
      "scores at B=0, after softmax: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7729, 0.2271, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2585, 0.4134, 0.3282, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2597, 0.1817, 0.2989, 0.2597, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1337, 0.2643, 0.2874, 0.1337, 0.1809, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1886, 0.1241, 0.1288, 0.1886, 0.2273, 0.1425, 0.0000, 0.0000],\n",
      "        [0.1166, 0.1226, 0.1751, 0.1166, 0.1353, 0.1149, 0.2189, 0.0000],\n",
      "        [0.1504, 0.1052, 0.1731, 0.1504, 0.0852, 0.1116, 0.0737, 0.1504]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " ---\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### 1) q, k, v linear transformation matrices\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self,context):\n",
    "        q = self.query(context) # dimension: (B, T, C)\n",
    "        k = self.key(context)   # dimension: (B, T, C)\n",
    "        v = self.value(context) # dimension: (B, T, C)\n",
    "\n",
    "        ### 2) Q @ K^T\n",
    "        # We need to keep the first dimension to be the batch size: \"B\"\n",
    "        # PyTorch will be able to perform matmul in parallel (at different batch indices)\n",
    "        # We need to transpose the other two indices (-2 and 01)\n",
    "        \n",
    "        print('q.shape,k.shape=',q.shape,k.shape)\n",
    "        k_transposed = k.transpose(-2, -1) #swaps the dim \"-2\" with dim \"-1\"\n",
    "        print('k_transposed.shape=',k_transposed.shape)\n",
    "        \n",
    "        ### 2-3) scaling the attention scores\n",
    "        embed_dim = q.shape[-1]\n",
    "        scores = q @ k_transposed * (embed_dim ** -0.5)  # Scale by sqrt(d_k) # [B, T, T]\n",
    "\n",
    "        ### 4) Masking\n",
    "        # We want to mask the next tokens from affecting the current token\n",
    "        # at i, we have to mask j>i --> similar to a lower triangular matrix\n",
    "        # We set them to -inf so after softmax, they become 0\n",
    "        \n",
    "        print('---')\n",
    "        print('scores at B=0, before masking: \\n', scores[0,:,:],'\\n ---')\n",
    "        seq_size = q.shape[-2]\n",
    "        self.tril=torch.tril(torch.ones(seq_size, seq_size))\n",
    "        \n",
    "        scores = scores.masked_fill(\n",
    "            self.tril[:seq_size, :seq_size] == 0, float('-inf')\n",
    "            ) # (B, T, T)\n",
    "        print('scores at B=0, after masking: \\n', scores[0,:,:],'\\n ---')\n",
    "\n",
    "        ### 5) applying the softmax\n",
    "        # scores has the dimension [B, T, T]\n",
    "        # we want to normalize the score for each query position independently\n",
    "        # so, softmax is applied along the last dimension\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1) #[B, T, T]\n",
    "        print('scores at B=0, after softmax: \\n', scores[0,:,:],'\\n ---')\n",
    "\n",
    "        ### 6) multiplying by v matrix: [B, T, C]\n",
    "        scores = scores @ v\n",
    "\n",
    "        return scores # B, T, C]\n",
    "\n",
    "SA=SelfAttention(embed_dim)\n",
    "scores=SA(org_embd_context)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-head attention\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/mha.png\" width=\"250\" alt=\"Multi-head Attention\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Specify the number of heads `n_heads` \\\n",
    "2- Create a SingleHeadAttention where we split the Q, K, V matrices, along embed_dim, into `n_heads` \\\n",
    "3- Concatanate the Value matrices \\\n",
    "    > 3a- First create a list of the heads in initialize, using torch.Module() \\\n",
    "    > 3b- concatanate them in the `forward` method \\\n",
    "4- Add an additional Linear layer on the top of it (called projection) \\\n",
    "5- Create a dropout layer\n",
    "\n",
    "This allows the PyTorch to be able to track the model and parameters, etc better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Specify the number of heads\n",
    "n_heads = 5\n",
    "head_size = embed_dim // 5\n",
    "\n",
    "dropout = 0. # The dropout fraction!\n",
    "\n",
    "# 2) Single head attention\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.key   = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "\n",
    "    def forward(self,context):\n",
    "        q = self.query(context) # dimension: [B, T, C']\n",
    "        k = self.key(context)   # dimension: [B, T, C']\n",
    "        v = self.value(context) # dimension: [B, T, C']\n",
    "\n",
    "        seq_size  = q.shape[-2]\n",
    "        embed_dim = q.shape[-1]\n",
    "\n",
    "        k_transposed = k.transpose(-2, -1) #swaps the dim \"-2\" with dim \"-1\"\n",
    "        scores = q @ k_transposed * (embed_dim ** -0.5)  # Scale by sqrt(d_k) # [B, T, T]\n",
    "        \n",
    "        self.tril=torch.tril(torch.ones(seq_size, seq_size)).to(scores.device)\n",
    "        scores = scores.masked_fill(\n",
    "            self.tril[:seq_size, :seq_size] == 0, float('-inf')\n",
    "            ) # (B, T, T)\n",
    "        \n",
    "        scores = F.softmax(scores, dim=-1) #[B, T, T]\n",
    "        scores = scores @ v #[B, T, T] x [B, T, C'] = [B, T, C']\n",
    "\n",
    "        return scores #[B, T, C']\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3a) Creating a list of SingleHeadAttention\n",
    "        # Not just a simple list, but a ModuleList. \n",
    "        # It allows the model to track the parameters\n",
    "        head_size = embed_dim//n_heads\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(embed_dim,head_size) \n",
    "                                for _ in range(n_heads)])\n",
    "        \n",
    "        # 4) Adding an additional linear layer (called projection)\n",
    "        # Will take the concatanated attentions [B,T,C] \n",
    "        # Its output should have the same dimension as the embedded context [B, T, C]\n",
    "        self.proj  = nn.Linear(embed_dim,embed_dim)\n",
    "\n",
    "        # 5) Adding the residual with dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,context):\n",
    "\n",
    "        # 3b) concatanating the list of SingleHeads modules\n",
    "        heads_list=[]\n",
    "        for head in self.heads:\n",
    "            heads_list.append(head(context))\n",
    "        heads = torch.cat(heads_list,dim=-1)\n",
    "        #print(f'heads.shape={heads.shape}\\n ---')\n",
    "\n",
    "        # 4) adding the linear layer, called projection\n",
    "        scores=self.proj(heads) # now score is [B, T, C]\n",
    "\n",
    "        # 5) adding the dropout layer\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        return scores #[B, T, C]\n",
    "      \n",
    "MHA = MultiHeadAttention(embed_dim,n_heads)\n",
    "scores = MHA(org_embd_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. FeedForward Layer\n",
    "A simple 2-layer network to add non-linearity and complexity to the system\n",
    "\n",
    "1- Increasing the embedding dimension by 4x \\\n",
    "2- Applying ReLU \\\n",
    "3- Decreasing the embedding dimension by 4x \\\n",
    "4- Adding a Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # The input to the network is [B, T, C]\n",
    "        self.network=nn.Sequential(\n",
    "            # 1) Increasing the dimension to [B, T, 4C]\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            #\n",
    "            # 2) ReLU\n",
    "            nn.ReLU(),\n",
    "            #\n",
    "            # 3) decreasing the dimension back to [B, T, C]\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "            #\n",
    "            # 4) Adding the dropout\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, scores):\n",
    "        return self.network(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Putting together the attention block!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to understand:\n",
    "**What is the Normalizing layer?**\n",
    "\n",
    "It normalizes the output of the previous layer (to have a mean of 0 and a standard deviation of 1) before going to the next layer.\n",
    "- Stabilizes the learning process\n",
    "- Prevents the exploding/vanishing gradients\n",
    "\n",
    "Although we used Softmax in the self-attention, the Value matrix values may be too large, so we need to normalize the output of the self-attention -> LayerNorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to put together the attention block:**\n",
    "\n",
    "1- Instantiating the MHA \\\n",
    "2- Creating the layer normalization LN1 to take the \"embedded context\" as residual and add to the attention scores \\\n",
    "3- Instantiating the FF \\\n",
    "4- Creating the normalizing layer2 to take the MHA output as residual and add to the FF outputs \\\n",
    "5- Connecting the normalizing layer 1 and MHA output\\\n",
    "6- Connecting the normalizing layer 2 and FF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Instantiating the MHA \n",
    "        # Input : [B, T, C]\n",
    "        # Output: [B, T, C]\n",
    "        self.mha = MultiHeadAttention(embed_dim,n_heads)\n",
    "\n",
    "        # 2) Creating the layer normalization module 1\n",
    "        # Input: Embedded context [B, T, C]\n",
    "        # Output: will be summed up with the MHA output [B, T, C]\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 3) Instantiating the FF\n",
    "        self.ff = FeedForward(embed_dim)\n",
    "\n",
    "        # 4) Creating the layer normalization module 2\n",
    "        # Input: Output of MHA [B, T, C]\n",
    "        # Output: will be summed up with the FF output [B, T, C]\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 5) Connecting layer norm 1 and MHA\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "\n",
    "        #6) Connecting layer norm 2 and FeedForward\n",
    "        return x + self.ff(self.ln2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Finalizing the Model\n",
    "\n",
    "1- Instantiating the Token Embedding \\\n",
    "2- Also, Instantiating a similar Embedding for Positional Embedding (instead of sin encoding) \\\n",
    "3- Instantiating the attention blocks n_layer times \\\n",
    "4- Adding a Layer Normalization at the end \\\n",
    "5- Adding a Linear Layer at the end to go from embed_dim to vocab_size \\\n",
    "6- Applying the softmax\n",
    "* Note: In training we don't need to apply the softmax directly, because CrossEntropy does this for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "seq_size=16\n",
    "embed_dim=128\n",
    "n_heads=4\n",
    "n_layers=4\n",
    "dropout=0.\n",
    "learning_rate = 0.0005\n",
    "n_epoch = 2000\n",
    "print_interval=100\n",
    "eval_iters=200\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Instantiating Token embedding\n",
    "        self.TokenEmbedding = nn.Embedding(vocab_size, embed_dim) # output: [B,T,C]\n",
    "\n",
    "        # 2) Instantiating Positional embedding\n",
    "        self.PositionalEmbedding = nn.Embedding(seq_size, embed_dim) #output: [T,C]\n",
    "\n",
    "        # 3) Instantiating Attention block\n",
    "        # Creating a list of Block modules\n",
    "        block_list = []\n",
    "        for _ in range(n_layers):\n",
    "            block = AttentionBlock(embed_dim, n_heads)\n",
    "            block_list.append(block)\n",
    "        # Convert the list into a Sequential container\n",
    "        self.blocks = nn.Sequential(*block_list) # * unpacks the list\n",
    "\n",
    "        # 4) Last Layer Normalization module\n",
    "        self.lnn = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 5) Last Linear layer to go from [B,T,C] to [B,T,vocan_size]\n",
    "        self.linearn = nn.Linear(embed_dim,vocab_size)\n",
    "    \n",
    "    def forward(self,context, targets=None):\n",
    "\n",
    "        context = context.to(device)\n",
    "        if targets is not None: targets = targets.to(device)\n",
    "        \n",
    "        batch_size = context.shape[0]\n",
    "        seq_size   = context.shape[1]\n",
    "        #vocab_size from the global\n",
    "\n",
    "        \n",
    "        # 1) Token embedding\n",
    "        tm = self.TokenEmbedding(context) # [B, T, C]\n",
    "\n",
    "        # 2) Positional embedding\n",
    "        # create a sequence of int numbers of 0 to vocab_size-1\n",
    "        tmp = torch.arange(seq_size, dtype=torch.int64).to(device) \n",
    "        pm  = self.PositionalEmbedding(tmp)\n",
    "\n",
    "        # Adding the two embeddings\n",
    "        x = tm + pm\n",
    "\n",
    "        # 3) Calculate the blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # 4 and 5) Pass the data through lnn and last linear layer\n",
    "        x = self.lnn(x)\n",
    "        y = self.linearn(x) # output: [B, T, vocab_size]\n",
    "\n",
    "        # If there are targets, it's training otherwise is inference\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            y       = y.view(batch_size*seq_size, vocab_size)\n",
    "            targets = targets.view(batch_size*seq_size)\n",
    "            loss    = F.cross_entropy(y, targets)\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def generation(self, context, max_tokens):\n",
    "        # context has dimensions of [B, T]\n",
    "        for _ in range(max_tokens):\n",
    "            # make sure the context fits in the sequence length\n",
    "            context_crop = context[:, -seq_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            y, loss = self(context_crop)\n",
    "\n",
    "            # focus only on the last token\n",
    "            y = y[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(y, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append the sample to the running sequence\n",
    "            context = torch.cat((context, next_token), dim=1) # (B, T+1)\n",
    "        return context       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num trainable params = 50562113\n",
      "step 0: train loss 4.341283321380615, val loss 4.310567378997803\n",
      "step 100: train loss 2.7017595767974854, val loss 2.7457518577575684\n",
      "step 200: train loss 2.4659042358398438, val loss 2.4751968383789062\n",
      "step 300: train loss 2.397965431213379, val loss 2.4199090003967285\n",
      "step 400: train loss 2.2674529552459717, val loss 2.3099539279937744\n",
      "step 500: train loss 2.0826053619384766, val loss 2.1047134399414062\n",
      "step 600: train loss 1.9665982723236084, val loss 2.0706069469451904\n",
      "step 700: train loss 1.8874051570892334, val loss 2.0652308464050293\n",
      "step 800: train loss 1.705950140953064, val loss 1.8789327144622803\n",
      "step 900: train loss 1.6663309335708618, val loss 1.8784754276275635\n",
      "step 1000: train loss 1.5862375497817993, val loss 1.887750506401062\n",
      "step 1100: train loss 1.6343244314193726, val loss 1.884762167930603\n",
      "step 1200: train loss 1.6150202751159668, val loss 1.836326003074646\n",
      "step 1300: train loss 1.500004529953003, val loss 1.6882438659667969\n",
      "step 1400: train loss 1.5141839981079102, val loss 1.701988697052002\n",
      "step 1500: train loss 1.5420805215835571, val loss 1.6673948764801025\n",
      "step 1600: train loss 1.4793388843536377, val loss 1.7104171514511108\n",
      "step 1700: train loss 1.4493262767791748, val loss 1.5847933292388916\n",
      "step 1800: train loss 1.4259549379348755, val loss 1.5429919958114624\n",
      "step 1900: train loss 1.4157452583312988, val loss 1.5618629455566406\n",
      "step 2000: train loss 1.333014726638794, val loss 1.6902021169662476\n",
      "step 2100: train loss 1.3538563251495361, val loss 1.614455223083496\n",
      "step 2200: train loss 1.291263461112976, val loss 1.5864315032958984\n",
      "step 2300: train loss 1.3640567064285278, val loss 1.6863147020339966\n",
      "step 2400: train loss 1.2398771047592163, val loss 1.5239062309265137\n",
      "step 2500: train loss 1.2949718236923218, val loss 1.5898473262786865\n",
      "step 2600: train loss 1.3471075296401978, val loss 1.50187349319458\n",
      "step 2700: train loss 1.2358790636062622, val loss 1.4985936880111694\n",
      "step 2800: train loss 1.2279000282287598, val loss 1.5778999328613281\n",
      "step 2900: train loss 1.2892605066299438, val loss 1.5393067598342896\n",
      "step 3000: train loss 1.2839679718017578, val loss 1.5454974174499512\n",
      "step 3100: train loss 1.286034107208252, val loss 1.53355073928833\n",
      "step 3200: train loss 1.1622803211212158, val loss 1.5571779012680054\n",
      "step 3300: train loss 1.244720220565796, val loss 1.5415915250778198\n",
      "step 3400: train loss 1.1829924583435059, val loss 1.6051554679870605\n",
      "step 3500: train loss 1.1580864191055298, val loss 1.6655364036560059\n",
      "step 3600: train loss 1.2250828742980957, val loss 1.4454050064086914\n",
      "step 3700: train loss 1.195315957069397, val loss 1.6456704139709473\n",
      "step 3800: train loss 1.102018117904663, val loss 1.4751734733581543\n",
      "step 3900: train loss 1.2426087856292725, val loss 1.5870457887649536\n",
      "step 3999: train loss 1.1661065816879272, val loss 1.6711962223052979\n",
      "Training took 1642.509127855301 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finetuning parameters to improve training loss\n",
    "batch_size=16\n",
    "seq_size=128\n",
    "embed_dim=256\n",
    "n_layers=64\n",
    "n_epoch = 4000\n",
    "\n",
    "# Instantiating the Transformer module\n",
    "model = Transformer(embed_dim,n_heads)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device ='cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(f\"num trainable params = {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "start=time.time()\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    # Get a batch of training data\n",
    "    context,target = get_batch('train')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Pass through the model (context,target) and calculate train loss\n",
    "    y,loss = model(context,target)\n",
    "    \n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        # Get a batch of validation data\n",
    "        context,target = get_batch('val')\n",
    "        \n",
    "        # Pass through the model (context,target) and calculate val loss\n",
    "        y,val_loss = model(context,target)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % print_interval == 0 or epoch == n_epoch - 1:\n",
    "        # Print the training and validation loss (NOTE: estimate_loss function was not created because loss is already calculated in forward function)\n",
    "        print(f\"step {epoch}: train loss {loss}, val loss {val_loss}\")\n",
    "\n",
    "print(f'Training took {time.time()-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 132.76370120048523 seconds\n",
      "---\n",
      "\n",
      "\n",
      "TRANIO:\n",
      "'Tis never, no man have the way.\n",
      "\n",
      "PARIS:\n",
      "Pray her i' the pluxes my worthy slafe.\n",
      "Heaven since it is in disdemptress should fall Pestate:\n",
      "I am for orced in bired past again, for that won again.\n",
      "\n",
      "MaRCIUS:\n",
      "He hath ever not our dream'st.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Tis nothing the very liege, nwhe'er down:\n",
      "I beseech thee, my lord, good even again\n",
      "As they shall lie in his own take.\n",
      "\n",
      "GLOUCESTER:\n",
      "I do lentD him: I am in so liet,\n",
      "Thou do'st prepare alevise it in death;\n",
      "But by dinner thousand lands is an yet then\n",
      "To let me ears and uple. But I bet a, by life:\n",
      "I beseech it billes to dance is better than\n",
      "True grown surr it; or elect him I untoN,\n",
      "Stanl'd the suggries to thy pridar,\n",
      "Broke all rapy my lord, or else\n",
      "Walke the clock and in this space.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "K, gracious lady, thou not be thy rarest,\n",
      "By thy folfing throne's Apollo queen.\n",
      "To Lord, which 'tis doobes some more casnive\n",
      "And the base against the accusary:\n",
      "And tlest where then that of half torngemn'd\n",
      "Be joint and reprevent corns till\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "start=time.time()\n",
    "context  = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "response = model.generation(context, max_tokens=1000)[0].tolist()\n",
    "print(f'Inference took {time.time()-start} seconds')\n",
    "print('---')\n",
    "print(decode(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the loss to about 1.1, I increased the training epochs to 4000 so the model can train for longer, increased the sequence size to 128 so the model can understand relationships between words in a sentence rather than just a few characters, increased the number of attention layers to increase model complexity (more trainable parameters), and increased the embedding dimension to capture more information. Model training took about 30 minutes and it had 50562113 trainable parameters. The text generated is a lot better than the previously generated text as it has more complete words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
