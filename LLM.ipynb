{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-12 14:51:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-11-12 14:51:04 (7.31 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n",
      "----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "----\n",
      "Length of dataset: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Let's download the tiny shakespeare dataset from Karpathy's github!\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Let's load the dataset into memory\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# let's look at the first 100 characters\n",
    "print('----')\n",
    "print(text[:100])\n",
    "\n",
    "# let's print the length of the dataset\n",
    "print('----')\n",
    "print(f\"Length of dataset: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize the dataset (Character-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "---\n",
      "Vocabulary size: 65 characters\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(set(text))\n",
    "vocab_size=len(chars)\n",
    "\n",
    "print(chars)\n",
    "print('---')\n",
    "print(f\"Vocabulary size: {vocab_size} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to map characters to integers and vice versa\n",
    "c2i={c:i for i, c in enumerate(chars)} # character to integer\n",
    "i2c={i:c for i, c in enumerate(chars)} # integer to character\n",
    "\n",
    "# encode a string to a \"list of integers\"\n",
    "encode=lambda s: [c2i[c] for c in s]\n",
    "\n",
    "# decode a list of integers to a string\n",
    "decode=lambda l: ''.join([i2c[i] for i in l])\n",
    "\n",
    "# Let's test it out\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=torch.Size([1115394]), data.dtype=torch.int64\n",
      "---\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset (and put it in a pytorch tensor)\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'data.shape={data.shape}, data.dtype={data.dtype}')\n",
    "print('---')\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a) Let's split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% - training set, 10% - validation set\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b) Let's see how to create input and targets during the training\n",
    "The goal of training a transformer language model is to predict \"the next token in the sequence\" given \"the current sequence of tokens\".\n",
    "\n",
    "**Example:**\n",
    "`Hello World! I am ChatGPT!`\n",
    "\n",
    "**Prediction:**\\\n",
    "Input: A sequence of tokens -> Output: The next token in the sequence\n",
    "___\n",
    "##### *Approach 1:*\n",
    "**Prediction 1:**\n",
    "Input: `[H, E, L, L, O, W, O, R]` -> Output: `[L]`\n",
    "___\n",
    "##### *Approach 2 (Sliding Window):*    \n",
    "**Prediction 1:**\n",
    "Input: `[H]` -> Output: `[L]`\n",
    "\n",
    "**Prediction 2:**\n",
    "Input: `[H, E]` -> Output: `[L]`\n",
    "\n",
    "**Prediction 3:**\n",
    "Input: `[H, E, L]` -> Output: `[L]`\n",
    "\n",
    "...\n",
    "\n",
    "**Prediction 8:**\n",
    "Input: `[H, E, L, L, O, W, O, R]` -> Output: `[L]`\n",
    "\n",
    "The sliding window approach:\n",
    "- Creates multiple training examples from a single input\n",
    "- While the sequence length is fixed, this teaches the model to handle contexts of different lengths -> critical for flexibility of the model\n",
    "- Can be done in parallel without pulling new data from the memory\n",
    "- Because of progressive learning, the training becomes more smooth and stable.\n",
    "- Improves the model understanding of contextual relationships between tokens\n",
    "\n",
    "___\n",
    "\n",
    "So, here to implement the sliding window, we create the input and output via the format below:\n",
    "- **Input:**`[H, E, L, L, O, W, O, R]`\n",
    "- **Target:** `[E, L, L, O, W, O, R, L]`\n",
    "\n",
    "We can see how this is actually implemented in the self-attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "seq_size=8, batch_size=4\n",
      "context.shape=torch.Size([4, 8]), target.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Define the sequence size\n",
    "# These are just small values for testing, later we will use a larger value\n",
    "seq_size   = 8 # how many characters are in a sequence\n",
    "batch_size = 4 # how many sequences in a batch\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # Randomly select starting indices for the batch (0, len(data)-seq_size)\n",
    "    # Do it for batch_size number of times and store in a torch vector\n",
    "    start_ind = torch.randint(0,len(data)-seq_size,(batch_size,))\n",
    "    #print(f'start_ind={start_ind}')\n",
    "    \n",
    "    context=[]\n",
    "    target=[]\n",
    "    for i in start_ind:\n",
    "        context.append(data[i  :i+seq_size  ])\n",
    "        target.append (data[i+1:i+seq_size+1])\n",
    "    \n",
    "    context = torch.stack(context) #converts from a list of tensors to a large tensor\n",
    "    target  = torch.stack(target)\n",
    "\n",
    "    return context, target\n",
    "\n",
    "context,target=get_batch('train')\n",
    "\n",
    "# Printing some properties of what we built\n",
    "print('---')\n",
    "print(f'seq_size={seq_size}, batch_size={batch_size}')\n",
    "print(f'context.shape={context.shape}, target.shape={target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=tensor([[ 1, 52, 53, 58,  1, 58, 46, 43],\n",
      "        [50, 40, 53, 63, 57,  1, 44, 56],\n",
      "        [ 1, 39, 52, 57, 61, 43, 56,  1],\n",
      "        [47, 58,  1, 61, 53, 59, 50, 42]])\n",
      "target =tensor([[52, 53, 58,  1, 58, 46, 43, 63],\n",
      "        [40, 53, 63, 57,  1, 44, 56, 53],\n",
      "        [39, 52, 57, 61, 43, 56,  1, 52],\n",
      "        [58,  1, 61, 53, 59, 50, 42,  1]])\n"
     ]
    }
   ],
   "source": [
    "print(f'context={context}')\n",
    "print(f'target ={target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: \" not the\"\n",
      "  --> n\n",
      " n --> o\n",
      " no --> t\n",
      " not -->  \n",
      " not  --> t\n",
      " not t --> h\n",
      " not th --> e\n",
      " not the --> y\n",
      "---in numbers---\n",
      "input: tensor([1]) --> output: 52\n",
      "input: tensor([ 1, 52]) --> output: 53\n",
      "input: tensor([ 1, 52, 53]) --> output: 58\n",
      "input: tensor([ 1, 52, 53, 58]) --> output: 1\n",
      "input: tensor([ 1, 52, 53, 58,  1]) --> output: 58\n",
      "input: tensor([ 1, 52, 53, 58,  1, 58]) --> output: 46\n",
      "input: tensor([ 1, 52, 53, 58,  1, 58, 46]) --> output: 43\n",
      "input: tensor([ 1, 52, 53, 58,  1, 58, 46, 43]) --> output: 63\n"
     ]
    }
   ],
   "source": [
    "b=0 #for example, at a fixed batch id\n",
    "print(f'sequence: \"{decode(context[b,:].tolist())}\"')\n",
    "for t in range(seq_size):\n",
    "    input_seq = context[b,:t+1].tolist()\n",
    "    output_seq=[target[b,t].tolist()]\n",
    "    print(f'{decode(input_seq)} --> {decode(output_seq)}')\n",
    "\n",
    "print('---in numbers---')\n",
    "for t in range(seq_size):\n",
    "    print(f'input: {context[b,:t+1]} --> output: {target[b,t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**From here, we will start building the blocks of the transformer!**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/transformer.png\" width=\"200\" alt=\"Transformer\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "*Input context* has the shape [Batch, Sequence Length] or `[B,T]`\n",
    "\n",
    "*Embedding tensor* would be of shape [Batch, Sequence Length, Channel] or `[B,T,C]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding\n",
    "Here, we are using a simple trainable embedding. It is essentially a look up table that for each \"token\", stores the \"embedding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        #\n",
    "        # nn.Embedding is a lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # It takes two arguments:\n",
    "        # 1. the size of the dictionary of embeddings: vocab_size  ; here we have 65 different tokens\n",
    "        # 2. the size of each embedding vector: embed_dim          ; how many numbers are used to describe each token\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    # taking the input tensor and returning the embedded tensor\n",
    "    def forward(self,input_context):\n",
    "        embedding_table=self.token_embedding_table(input_context)\n",
    "        #\n",
    "        print(f'input_contex.shape={input_context.shape}')\n",
    "        print(f'embedding_table.shape={embedding_table.shape}')\n",
    "\n",
    "        return embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_contex.shape=torch.Size([4, 8])\n",
      "embedding_table.shape=torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary in the our text (all unique characters, words, ...)\n",
    "# Determined earlier vocab_size=65\n",
    "\n",
    "# Embedding dimension\n",
    "embed_dim=65\n",
    "\n",
    "# using above class to create the embedding\n",
    "org_embedding    = TokenEmbedding(vocab_size,embed_dim)\n",
    "org_embd_context = org_embedding(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Self attention\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/sa.png\" width=\"150\" alt=\"Self Attention\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inputs to the SelfAttention:* Embedded context `[B,T,C]`\n",
    "\n",
    "1- Create **linear transformation matrices** for **Key, Query, Value** matrices.\\\n",
    "*Note:* We build the $W_K$, $W_Q$, $W_V$ matrices for all tokens in **a sequence**.\\\n",
    "These are of shape `[C,C]` and applied in parallel across the batch.\\\n",
    "\n",
    "2- Use **Query** and **Key** matrices to create the base attention score\\\n",
    "*Note:* $Q$ = $W_Q$ * (Embedded context), $K$ = $W_K$ * (Embedded context)\\\n",
    "$Q$ and $K$ have shape `[B,T,C] @ [C,C] = [B,T,C]`\n",
    "\n",
    "base_scores = `Q @ K.T` will be of dimension: `[B,T,C]*[B,C,T] = [B,T,T]`\n",
    "\n",
    "3- Scale it with the $\\sqrt{d_{emb}}$, where $d_{emb}$ in the code is referred to as `C` (Channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Let's talk about Masking:** Let's revisit our input before entering the transformer block. Imagine B=1:\n",
    "\n",
    "context input: `[H, E, L, L, O, W, O, R]`\\\n",
    "target output: `[E, L, L, O, W, O, R, L]`\n",
    "\n",
    "```\n",
    "H -> [0.1, 0.2, ..., 0.0]  # C=65 numbers\\\n",
    "E -> [0.3, 0.1, ..., 0.2]  # C=65 numbers\\\n",
    "context = [\n",
    "    [0.1, 0.2, ..., 0.0],  # H\n",
    "    [0.3, 0.1, ..., 0.2],  # E\n",
    "    [...],                  # L\n",
    "    [...],                  # L\n",
    "    [...],                  # O\n",
    "    [...],                  # W\n",
    "    [...],                  # O\n",
    "    [...],                  # R\n",
    "]  # shape: [B=1, T=8, C=65]\n",
    "\n",
    "Q = context @ Wq  # [1, 8, 65]\n",
    "K = context @ Wk  # [1, 8, 65]\n",
    "\n",
    "scores = [\n",
    "    # H    E    L    L    O    W    O    R\n",
    "    [H·H, H·E, H·L, H·L, H·O, H·W, H·O, H·R],  # H's interactions\n",
    "    [E·H, E·E, E·L, E·L, E·O, E·W, E·O, E·R],  # E's interactions\n",
    "    [L·H, L·E, L·L, L·L, L·O, L·W, L·O, L·R],  # L's interactions\n",
    "    [L·H, L·E, L·L, L·L, L·O, L·W, L·O, L·R],  # L's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O, O·R],  # O's interactions\n",
    "    [W·H, W·E, W·L, W·L, W·O, W·W, W·O, W·R],  # W's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O, O·R],  # O's interactions\n",
    "    [R·H, R·E, R·L, R·L, R·O, R·W, R·O, R·R],  # R's interactions\n",
    "]  # shape: [1, 8, 8] or [B, T, T]\n",
    "```\n",
    "Remember that we want to have predictions for the next token via a sliding window approach:\n",
    "- At step 0 (first row), we want the input to be `[H]` and the target to be `[E]`\n",
    "- At step 1 (second row), we want the input to be `[H, E]` and the target to be `[L]`\n",
    "- ...\n",
    "- At step 7 (last row), we want the input to be `[H, E, L, L, O, W, O, R]` and the target to be `[L]`\n",
    "\n",
    "So, we essentially want the scores to be like:\n",
    "```\n",
    "scores = [\n",
    "    # H    E    L    L    O    W    O    R\n",
    "    [H·H, 0  ,  0  , 0  , 0  , 0  , 0  , 0 ],  # H's interactions\n",
    "    [E·H, E·E,  0  , 0  , 0  , 0  , 0  , 0 ],  # E's interactions\n",
    "    [L·H, L·E, L·L,  0  , 0  , 0  , 0  , 0 ],  # L's interactions\n",
    "    [L·H, L·E, L·L, L·L,  0  , 0  , 0  , 0 ],  # L's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O,  0  , 0  , 0 ],  # O's interactions\n",
    "    [W·H, W·E, W·L, W·L, W·O, W·W,  0  , 0 ],  # W's interactions\n",
    "    [O·H, O·E, O·L, O·L, O·O, O·W, O·O,  0 ],  # O's interactions\n",
    "    [R·H, R·E, R·L, R·L, R·O, R·W, R·O, R·R],  # R's interactions\n",
    "]  # shape: [1, 8, 8] or [B, T, T]\n",
    "```\n",
    "This is why we need to mask the future tokens. Instead of setting them to 0, we set them to $-\\infty$ so that after the softmax, they become 0.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4- Masking the future tokens with $-\\infty$ \\\n",
    "5- Softmax:\n",
    "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "6- Multiply by Value matrix to create the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.shape,k.shape= torch.Size([4, 8, 65]) torch.Size([4, 8, 65])\n",
      "k_transposed.shape= torch.Size([4, 65, 8])\n",
      "---\n",
      "scores at B=0, before masking: \n",
      " tensor([[-4.5996e-01,  6.8997e-01, -5.2011e-01, -4.3751e-01, -4.5996e-01,\n",
      "         -4.3751e-01, -3.0724e-01,  8.6585e-02],\n",
      "        [-3.7844e-01,  2.4135e-02,  1.4577e-01,  5.0176e-02, -3.7844e-01,\n",
      "          5.0176e-02, -2.1089e-01, -1.1838e-01],\n",
      "        [ 2.8905e-01, -1.9408e-01, -2.9102e-01,  5.0614e-01,  2.8905e-01,\n",
      "          5.0614e-01,  9.8043e-01,  6.7601e-02],\n",
      "        [ 2.8141e-01,  1.6873e-01, -2.9702e-01, -2.4251e-01,  2.8141e-01,\n",
      "         -2.4251e-01, -6.0282e-01, -9.0550e-05],\n",
      "        [-4.5996e-01,  6.8997e-01, -5.2011e-01, -4.3751e-01, -4.5996e-01,\n",
      "         -4.3751e-01, -3.0724e-01,  8.6585e-02],\n",
      "        [ 2.8141e-01,  1.6873e-01, -2.9702e-01, -2.4251e-01,  2.8141e-01,\n",
      "         -2.4251e-01, -6.0282e-01, -9.0550e-05],\n",
      "        [ 1.0262e-01, -6.1391e-02,  2.1577e-01,  1.2711e-01,  1.0262e-01,\n",
      "          1.2711e-01, -3.7851e-02, -4.4804e-01],\n",
      "        [-4.5867e-02,  5.3020e-02, -1.2831e-01,  8.4091e-02, -4.5867e-02,\n",
      "          8.4091e-02, -2.3888e-01,  1.2142e-01]], grad_fn=<SliceBackward0>) \n",
      " ---\n",
      "scores at B=0, after masking: \n",
      " tensor([[-0.4600,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3784,  0.0241,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2890, -0.1941, -0.2910,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2814,  0.1687, -0.2970, -0.2425,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.4600,  0.6900, -0.5201, -0.4375, -0.4600,    -inf,    -inf,    -inf],\n",
      "        [ 0.2814,  0.1687, -0.2970, -0.2425,  0.2814, -0.2425,    -inf,    -inf],\n",
      "        [ 0.1026, -0.0614,  0.2158,  0.1271,  0.1026,  0.1271, -0.0379,    -inf],\n",
      "        [-0.0459,  0.0530, -0.1283,  0.0841, -0.0459,  0.0841, -0.2389,  0.1214]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " ---\n",
      "scores at B=0, after softmax: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4007, 0.5993, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4594, 0.2834, 0.2572, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3283, 0.2933, 0.1841, 0.1944, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1404, 0.4434, 0.1322, 0.1436, 0.1404, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2156, 0.1926, 0.1209, 0.1277, 0.2156, 0.1277, 0.0000, 0.0000],\n",
      "        [0.1452, 0.1232, 0.1626, 0.1488, 0.1452, 0.1488, 0.1262, 0.0000],\n",
      "        [0.1204, 0.1329, 0.1108, 0.1371, 0.1204, 0.1371, 0.0992, 0.1423]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " ---\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### 1) q, k, v linear transformation matrices\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self,context):\n",
    "        q = self.query(context) # dimension: (B, T, C)\n",
    "        k = self.key(context)   # dimension: (B, T, C)\n",
    "        v = self.value(context) # dimension: (B, T, C)\n",
    "\n",
    "        ### 2) Q @ K^T\n",
    "        # We need to keep the first dimension to be the batch size: \"B\"\n",
    "        # PyTorch will be able to perform matmul in parallel (at different batch indices)\n",
    "        # We need to transpose the other two indices (-2 and 01)\n",
    "        \n",
    "        print('q.shape,k.shape=',q.shape,k.shape)\n",
    "        k_transposed = k.transpose(-2, -1) #swaps the dim \"-2\" with dim \"-1\"\n",
    "        print('k_transposed.shape=',k_transposed.shape)\n",
    "        \n",
    "        ### 2-3) scaling the attention scores\n",
    "        embed_dim = q.shape[-1]\n",
    "        scores = q @ k_transposed * (embed_dim ** -0.5)  # Scale by sqrt(d_k) # [B, T, T]\n",
    "\n",
    "        ### 4) Masking\n",
    "        # We want to mask the next tokens from affecting the current token\n",
    "        # at i, we have to mask j>i --> similar to a lower triangular matrix\n",
    "        # We set them to -inf so after softmax, they become 0\n",
    "        \n",
    "        print('---')\n",
    "        print('scores at B=0, before masking: \\n', scores[0,:,:],'\\n ---')\n",
    "        seq_size = q.shape[-2]\n",
    "        self.tril=torch.tril(torch.ones(seq_size, seq_size))\n",
    "        \n",
    "        scores = scores.masked_fill(\n",
    "            self.tril[:seq_size, :seq_size] == 0, float('-inf')\n",
    "            ) # (B, T, T)\n",
    "        print('scores at B=0, after masking: \\n', scores[0,:,:],'\\n ---')\n",
    "\n",
    "        ### 5) applying the softmax\n",
    "        # scores has the dimension [B, T, T]\n",
    "        # we want to normalize the score for each query position independently\n",
    "        # so, softmax is applied along the last dimension\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1) #[B, T, T]\n",
    "        print('scores at B=0, after softmax: \\n', scores[0,:,:],'\\n ---')\n",
    "\n",
    "        ### 6) multiplying by v matrix: [B, T, C]\n",
    "        scores = scores @ v\n",
    "\n",
    "        return scores # B, T, C]\n",
    "\n",
    "SA=SelfAttention(embed_dim)\n",
    "scores=SA(org_embd_context)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-head attention\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/mha.png\" width=\"250\" alt=\"Multi-head Attention\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Specify the number of heads `n_heads` \\\n",
    "2- Create a SingleHeadAttention where we split the Q, K, V matrices, along embed_dim, into `n_heads` \\\n",
    "3- Concatanate the Value matrices \\\n",
    "    > 3a- First create a list of the heads in initialize, using torch.Module() \\\n",
    "    > 3b- concatanate them in the `forward` method \\\n",
    "4- Add an additional Linear layer on the top of it (called projection) \\\n",
    "5- Create a dropout layer\n",
    "\n",
    "This allows the PyTorch to be able to track the model and parameters, etc better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Specify the number of heads\n",
    "n_heads = 5\n",
    "head_size = embed_dim // 5\n",
    "\n",
    "dropout = 0. # The dropout fraction!\n",
    "\n",
    "# 2) Single head attention\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.key   = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "\n",
    "    def forward(self,context):\n",
    "        q = self.query(context) # dimension: [B, T, C']\n",
    "        k = self.key(context)   # dimension: [B, T, C']\n",
    "        v = self.value(context) # dimension: [B, T, C']\n",
    "\n",
    "        seq_size  = q.shape[-2]\n",
    "        embed_dim = q.shape[-1]\n",
    "\n",
    "        k_transposed = k.transpose(-2, -1) #swaps the dim \"-2\" with dim \"-1\"\n",
    "        scores = q @ k_transposed * (embed_dim ** -0.5)  # Scale by sqrt(d_k) # [B, T, T]\n",
    "        \n",
    "        self.tril=torch.tril(torch.ones(seq_size, seq_size)).to(scores.device)\n",
    "        scores = scores.masked_fill(\n",
    "            self.tril[:seq_size, :seq_size] == 0, float('-inf')\n",
    "            ) # (B, T, T)\n",
    "        \n",
    "        scores = F.softmax(scores, dim=-1) #[B, T, T]\n",
    "        scores = scores @ v #[B, T, T] x [B, T, C'] = [B, T, C']\n",
    "\n",
    "        return scores #[B, T, C']\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3a) Creating a list of SingleHeadAttention\n",
    "        # Not just a simple list, but a ModuleList. \n",
    "        # It allows the model to track the parameters\n",
    "        head_size = embed_dim//n_heads\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(embed_dim,head_size) \n",
    "                                for _ in range(n_heads)])\n",
    "        \n",
    "        # 4) Adding an additional linear layer (called projection)\n",
    "        # Will take the concatanated attentions [B,T,C] \n",
    "        # Its output should have the same dimension as the embedded context [B, T, C]\n",
    "        self.proj  = nn.Linear(embed_dim,embed_dim)\n",
    "\n",
    "        # 5) Adding the residual with dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,context):\n",
    "\n",
    "        # 3b) concatanating the list of SingleHeads modules\n",
    "        heads_list=[]\n",
    "        for head in self.heads:\n",
    "            heads_list.append(head(context))\n",
    "        heads = torch.cat(heads_list,dim=-1)\n",
    "        #print(f'heads.shape={heads.shape}\\n ---')\n",
    "\n",
    "        # 4) adding the linear layer, called projection\n",
    "        scores=self.proj(heads) # now score is [B, T, C]\n",
    "\n",
    "        # 5) adding the dropout layer\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        return scores #[B, T, C]\n",
    "      \n",
    "MHA = MultiHeadAttention(embed_dim,n_heads)\n",
    "scores = MHA(org_embd_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. FeedForward Layer\n",
    "A simple 2-layer network to add non-linearity and complexity to the system\n",
    "\n",
    "1- Increasing the embedding dimension by 4x \\\n",
    "2- Applying ReLU \\\n",
    "3- Decreasing the embedding dimension by 4x \\\n",
    "4- Adding a Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # The input to the network is [B, T, C]\n",
    "        self.network=nn.Sequential(\n",
    "            # 1) Increasing the dimension to [B, T, 4C]\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            #\n",
    "            # 2) ReLU\n",
    "            nn.ReLU(),\n",
    "            #\n",
    "            # 3) decreasing the dimension back to [B, T, C]\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "            #\n",
    "            # 4) Adding the dropout\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, scores):\n",
    "        return self.network(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Putting together the attention block!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to understand:\n",
    "**What is the Normalizing layer?**\n",
    "\n",
    "It normalizes the output of the previous layer (to have a mean of 0 and a standard deviation of 1) before going to the next layer.\n",
    "- Stabilizes the learning process\n",
    "- Prevents the exploding/vanishing gradients\n",
    "\n",
    "Although we used Softmax in the self-attention, the Value matrix values may be too large, so we need to normalize the output of the self-attention -> LayerNorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to put together the attention block:**\n",
    "\n",
    "1- Instantiating the MHA \\\n",
    "2- Creating the layer normalization LN1 to take the \"embedded context\" as residual and add to the attention scores \\\n",
    "3- Instantiating the FF \\\n",
    "4- Creating the normalizing layer2 to take the MHA output as residual and add to the FF outputs \\\n",
    "5- Connecting the normalizing layer 1 and MHA output\\\n",
    "6- Connecting the normalizing layer 2 and FF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Instantiating the MHA \n",
    "        # Input : [B, T, C]\n",
    "        # Output: [B, T, C]\n",
    "        self.mha = MultiHeadAttention(embed_dim,n_heads)\n",
    "\n",
    "        # 2) Creating the layer normalization module 1\n",
    "        # Input: Embedded context [B, T, C]\n",
    "        # Output: will be summed up with the MHA output [B, T, C]\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 3) Instantiating the FF\n",
    "        self.ff = FeedForward(embed_dim)\n",
    "\n",
    "        # 4) Creating the layer normalization module 2\n",
    "        # Input: Output of MHA [B, T, C]\n",
    "        # Output: will be summed up with the FF output [B, T, C]\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 5) Connecting layer norm 1 and MHA\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "\n",
    "        #6) Connecting layer norm 2 and FeedForward\n",
    "        return x + self.ff(self.ln2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Finalizing the Model\n",
    "\n",
    "1- Instantiating the Token Embedding \\\n",
    "2- Also, Instantiating a similar Embedding for Positional Embedding (instead of sin encoding) \\\n",
    "3- Instantiating the attention blocks n_layer times \\\n",
    "4- Adding a Layer Normalization at the end \\\n",
    "5- Adding a Linear Layer at the end to go from embed_dim to vocab_size \\\n",
    "6- Applying the softmax\n",
    "* Note: In training we don't need to apply the softmax directly, because CrossEntropy does this for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "seq_size=16\n",
    "embed_dim=128\n",
    "n_heads=4\n",
    "n_layers=4\n",
    "dropout=0.\n",
    "learning_rate = 0.0005\n",
    "n_epoch = 2000\n",
    "print_interval=100\n",
    "eval_iters=200\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embed_dim,n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Instantiating Token embedding\n",
    "        self.TokenEmbedding = nn.Embedding(vocab_size, embed_dim) # output: [B,T,C]\n",
    "\n",
    "        # 2) Instantiating Positional embedding\n",
    "        self.PositionalEmbedding = nn.Embedding(seq_size, embed_dim) #output: [T,C]\n",
    "\n",
    "        # 3) Instantiating Attention block\n",
    "        # Creating a list of Block modules\n",
    "        block_list = []\n",
    "        for _ in range(n_layers):\n",
    "            block = AttentionBlock(embed_dim, n_heads)\n",
    "            block_list.append(block)\n",
    "        # Convert the list into a Sequential container\n",
    "        self.blocks = nn.Sequential(*block_list) # * unpacks the list\n",
    "\n",
    "        # 4) Last Layer Normalization module\n",
    "        self.lnn = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 5) Last Linear layer to go from [B,T,C] to [B,T,vocan_size]\n",
    "        self.linearn = nn.Linear(embed_dim,vocab_size)\n",
    "    \n",
    "    def forward(self,context, targets=None):\n",
    "\n",
    "        context = context.to(device)\n",
    "        if targets is not None: targets = targets.to(device)\n",
    "        \n",
    "        batch_size = context.shape[0]\n",
    "        seq_size   = context.shape[1]\n",
    "        #vocab_size from the global\n",
    "\n",
    "        \n",
    "        # 1) Token embedding\n",
    "        tm = self.TokenEmbedding(context) # [B, T, C]\n",
    "\n",
    "        # 2) Positional embedding\n",
    "        # create a sequence of int numbers of 0 to vocab_size-1\n",
    "        tmp = torch.arange(seq_size, dtype=torch.int64).to(device) \n",
    "        pm  = self.PositionalEmbedding(tmp)\n",
    "\n",
    "        # Adding the two embeddings\n",
    "        x = tm + pm\n",
    "\n",
    "        # 3) Calculate the blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # 4 and 5) Pass the data through lnn and last linear layer\n",
    "        x = self.lnn(x)\n",
    "        y = self.linearn(x) # output: [B, T, vocab_size]\n",
    "\n",
    "        # If there are targets, it's training otherwise is inference\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            y       = y.view(batch_size*seq_size, vocab_size)\n",
    "            targets = targets.view(batch_size*seq_size)\n",
    "            loss    = F.cross_entropy(y, targets)\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def generation(self, context, max_tokens):\n",
    "        # context has dimensions of [B, T]\n",
    "        for _ in range(max_tokens):\n",
    "            # make sure the context fits in the sequence length\n",
    "            context_crop = context[:, -seq_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            y, loss = self(context_crop)\n",
    "\n",
    "            # focus only on the last token\n",
    "            y = y[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(y, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append the sample to the running sequence\n",
    "            context = torch.cat((context, next_token), dim=1) # (B, T+1)\n",
    "        return context       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOMEWORK: Implement the estimate_loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Transformer module\n",
    "model = Transformer(embed_dim,n_heads)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device ='cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "## HOMEWORK: Print the number of parameters in the model\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "start=time.time()\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    # get a batch of data\n",
    "    \n",
    "\n",
    "    # pass through the model (context,target)\n",
    "    \n",
    "\n",
    "    if epoch % print_interval == 0 or epoch == n_epoch - 1:\n",
    "        ## HOMEWORK: Calculate the training and validation loss using the estimate_loss function\n",
    "        print(f\"step {epoch}: train loss {losses['train']}, val loss {losses['val']}\")\n",
    "\n",
    "    # calculate the gradients\n",
    "    \n",
    "\n",
    "    # backward propagations\n",
    "    \n",
    "\n",
    "    # Optimizer step\n",
    "    \n",
    "\n",
    "print(f'Training took {time.time()-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 7.255411148071289 seconds\n",
      "---\n",
      "\n",
      "And Bust thepwell, me.\n",
      "\n",
      "MONGERIO:\n",
      "O, your consaid,\n",
      "Was on redge on'sble your to:\n",
      "Twas and mustring, to cosery thee; I called\n",
      "To Bothan the hramelifter's stage? fill fend think's than. Will in the .\n",
      "\n",
      "GlOLISABETHERMIO:\n",
      "To is bid not!\n",
      "\n",
      "SeCame, good throped sumis shivil, that inray gralacemer o freardis\n",
      "God heartly brease-pook rit's dees med,\n",
      "Tell thy looded-teaven you her and not yone, geven the shall you, but son firmed?\n",
      "\n",
      "HENRIVALRENANE:\n",
      "But his tisted conders,\n",
      "Noyle saad sestand mine I trong peds doed Comorld.\n",
      "\n",
      "ESMOLI:\n",
      "Frieven to call seeper awere somear\n",
      "Will didses off ither heavarttent your go?\n",
      "\n",
      "Lecrupled shon ceives\n",
      "Evece, a meock of have comched that I slind the\n",
      "Jal is peopord I'll wan take heaved?\n",
      "Ahd mancloony name to hive scoces'd bown:\n",
      "God supos. KING'HALD EDICHANG:\n",
      "O, metto I have ear our or how aftents in beight.\n",
      "\n",
      "THAR HIONGABALLA:\n",
      "Woulk, marry the\n",
      "I gaintle hmeads becen ful my py fanlesse! I to we thee did;\n",
      "Undrencentusemough meecrousint lead, Come this anted tress replieding\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "start=time.time()\n",
    "context  = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "response = model.generation(context, max_tokens=1000)[0].tolist()\n",
    "print(f'Inference took {time.time()-start} seconds')\n",
    "print('---')\n",
    "print(decode(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
